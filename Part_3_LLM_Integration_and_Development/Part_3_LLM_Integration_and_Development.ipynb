{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d3c8bdf",
   "metadata": {},
   "source": [
    "# LLM Integration and Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea9423b",
   "metadata": {},
   "source": [
    "# Text Processing (with a language model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af5f5ee",
   "metadata": {},
   "source": [
    "Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6256a549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\msarv\\anaconda3\\lib\\site-packages (4.38.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\msarv\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\msarv\\anaconda3\\lib\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\msarv\\anaconda3\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\msarv\\anaconda3\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\msarv\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\msarv\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\msarv\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\msarv\\anaconda3\\lib\\site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\msarv\\anaconda3\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\msarv\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\msarv\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\msarv\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.6.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\msarv\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\msarv\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\msarv\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\msarv\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\msarv\\anaconda3\\lib\\site-packages (from requests->transformers) (2023.11.17)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24928f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\msarv\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Import Libraries\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba5e8e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading specific pre-trained models by name\n",
    "model_name = \"facebook/bart-large-cnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8afa695e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model for summarization\n",
    "summarization_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "summarization_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "070a63af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model for text generation\n",
    "text_generation_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "text_generation_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9d14444",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, let's define a news article for summarization:\n",
    "\n",
    "news_article = \"\"\"\n",
    "More than 50 members of the United Nations have joined the U.S. in pursuing a draft resolution to establish artificial intelligence (AI) safety guidelines. \n",
    "\n",
    "U.S. Ambassador Linda Thomas-Greenfield on Thursday read a statement that discussed the draft resolution titled \"Seizing the Opportunities of Safe, Secure, and Trustworthy Artificial Intelligence Systems for Sustainable Development,\" which would aim to \"articulate a shared approach to AI systems.\" \n",
    "\n",
    "\"The resolution calls on Member States to promote safe, secure, and trustworthy AI systems to address the world’s greatest challenges, including those related to poverty elimination, global health, food security, climate, energy, and education,\" Thomas-Greenfield said in a prepared statement. \n",
    "\"We are resolved to bridge the artificial intelligence and other digital divides between and within countries through capacity building, increasing digital literacy, and other actions,\" she added. \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f29ed80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: Draft resolution would aim to \"articulate a shared approach to AI systems\" U.S. Ambassador Linda Thomas-Greenfield: \"We are resolved to bridge the artificial intelligence and other digital divides between and within countries\"\n"
     ]
    }
   ],
   "source": [
    "#Now, we can perform text summarization using the BART model:\n",
    "\n",
    "# Text Summarization\n",
    "summarization = pipeline(\"summarization\", model=summarization_model, tokenizer=summarization_tokenizer)\n",
    "summary = summarization(news_article, max_length=150, min_length=30, do_sample=False)\n",
    "\n",
    "print(\"Summary:\", summary[0]['summary_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ebe356",
   "metadata": {},
   "source": [
    "This will generate a summary of the news article.\n",
    "Next, let's generate some creative text variations based on a seed sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d731979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed sentence\n",
    "seed_sentence = \"In a world where technology reigns supreme,\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a6c50f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'BartForConditionalGeneration' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "# Text Generation\n",
    "text_generation = pipeline(\"text-generation\", model=text_generation_model, tokenizer=text_generation_tokenizer)\n",
    "generated_text = text_generation(seed_sentence, max_length=100, num_return_sequences=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39861bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text Variations:\n",
      "Variation 1: In a world where technology reigns supreme,echnology reign’s supreme. a world in which technology is supreme, and where technology is the only thing that matters. in this world, technology is king, and technology is a way of life. in a world of technology, technology rules supreme, with no limits.\n",
      "Variation 2: In a world where technology reigns supreme,echnology reign’s supreme. a world in which technology is supreme, and where technology is the only thing that matters. in this world, technology is king, and technology is a way of life. in a world of technology, technology will rule supreme.\n",
      "Variation 3: In a world where technology reigns supreme,echnology reign’s supreme. a world in which technology is supreme, and where technology is the only thing that matters. in this world, technology is king, and technology is a way of life. in a world of technology, technology rules supreme.\n"
     ]
    }
   ],
   "source": [
    "print(\"Generated Text Variations:\")\n",
    "for i, text in enumerate(generated_text):\n",
    "    print(f\"Variation {i+1}: {text['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb32030",
   "metadata": {},
   "source": [
    "# Custom Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7679cc",
   "metadata": {},
   "source": [
    "# Use of Language Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f3f817",
   "metadata": {},
   "source": [
    "Utilizing the BART model from the transformers library to generate responses based on the provided prompts. This showcases how prompts can influence the language model's output to generate contextually relevant responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c29dcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries and Load BART Model\n",
    "import random\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8938489",
   "metadata": {},
   "source": [
    "# Clear and Effective Prompts:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7908c4e5",
   "metadata": {},
   "source": [
    " Our prompts dictionary contains clear and concise prompts for various scenarios such as general inquiries, technical support, feature requests, etc. These prompts are designed to elicit specific types of responses from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18923d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = {\n",
    "    \"General Inquiry\": \"Hello! How can I assist you today with our software products?\",\n",
    "    \"Technical Support\": \"Are you experiencing any technical issues with our software? Please describe the problem in detail.\",\n",
    "    \"Feature Requests\": \"Do you have any suggestions or feature requests for our software products? We value your feedback!\",\n",
    "    \"Product Information\": \"Would you like more information about a specific product or feature? Please specify, and I'll provide the details.\",\n",
    "    \"Account Inquiry\": \"For account-related inquiries or assistance with licensing, please provide your account details or license key.\",\n",
    "    \"Feedback\": \"We strive to provide excellent service! How would you rate your experience with our support today? Your feedback helps us improve.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e04601f",
   "metadata": {},
   "source": [
    "# Steering Model Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab87aeb",
   "metadata": {},
   "source": [
    "By providing different prompts for various scenarios, effectively steering the model's output towards generating responses tailored to each specific use case. For example, the prompt for technical support will likely result in responses focused on troubleshooting and problem-solving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83f4eb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt):\n",
    "    input_text = prompt\n",
    "    encoded_prompt = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=False)\n",
    "\n",
    "    # Generate response\n",
    "    output = model.generate(**encoded_prompt, max_length=100, num_return_sequences=1, temperature=0.7, top_k=50)\n",
    "\n",
    "    # Decode and return the response\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6b719e",
   "metadata": {},
   "source": [
    "# Example Usage:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0849d97b",
   "metadata": {},
   "source": [
    "The generate_response function takes a prompt as input, generates a response from the BART model, and returns the decoded response. This demonstrates how prompts can be used to interact with the language model and obtain responses customized to different scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1859cba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General Inquiry:\n",
      "Hello! How can I assist you today with our software products?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msarv\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot Response:\n",
      "Hello! How can I assist you today with our software products? Please email us at support@dailymail.co.uk for help with your software products. We are happy to help you with any questions you may have about our products. Please send us an email with your questions and we'll try to assist you.\n",
      "-------------------------------\n",
      "Technical Support:\n",
      "Are you experiencing any technical issues with our software? Please describe the problem in detail.\n",
      "Bot Response:\n",
      "Are you experiencing any technical issues with our software? Please describe the problem in detail. We will be happy to help you with any questions you may have about our software. Back to Mail Online home.back to the page you came from. Click here for more information.\n",
      "-------------------------------\n",
      "Feature Requests:\n",
      "Do you have any suggestions or feature requests for our software products? We value your feedback!\n",
      "Bot Response:\n",
      "Do you have any suggestions or feature requests for our software products? We value your feedback! Share your photos, videos and stories with CNN iReport. Send your photos and videos to jennifer.smith@dailymail.co.uk. Share your stories and videos with iReport and we'll feature them in next week's column.\n",
      "-------------------------------\n",
      "Product Information:\n",
      "Would you like more information about a specific product or feature? Please specify, and I'll provide the details.\n",
      "Bot Response:\n",
      "Would you like more information about a specific product or feature? Please specify, and I'll provide the details. Would you like to talk to a specific person or person about a particular product or product? Please tell us about your experience and we'll get back to you.\n",
      "-------------------------------\n",
      "Account Inquiry:\n",
      "For account-related inquiries or assistance with licensing, please provide your account details or license key.\n",
      "Bot Response:\n",
      "For account-related inquiries or assistance with licensing, please provide your account details or license key. For support on our site, please call the Samaritans on 08457 90 90 90 or visit a local Samaritans branch, see www.samaritans.org for details. In the U.S. call the National Suicide Prevention Line on 1-800-273-8255.\n",
      "-------------------------------\n",
      "Feedback:\n",
      "We strive to provide excellent service! How would you rate your experience with our support today? Your feedback helps us improve.\n",
      "Bot Response:\n",
      "We strive to provide excellent service! How would you rate your experience with our support today? Your feedback helps us improve. We are happy to respond to all of your questions and comments. Please share your feedback with us at the bottom of the page. We would love to hear from you.\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "for prompt_type, prompt_text in prompts.items():\n",
    "    print(prompt_type + \":\\n\" + prompt_text)\n",
    "    response = generate_response(prompt_text)\n",
    "    print(\"Bot Response:\")\n",
    "    print(response)\n",
    "    print(\"-------------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
